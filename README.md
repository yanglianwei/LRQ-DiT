# LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation


S1: create env by viditq-flux.yaml and viditq-pixart.yaml
S2: conda activate env 
S3: cd ./quant_utils    
S4: pip install -e .
S5: chmod u+x /path/………/example.sh

S6: ./example.sh



# Citation

If you find our work helpful, please consider citing:

```
@article{yang2025lrq,
  title={LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Image and Video Generation},
  author={Yang, Lianwei and Lin, Haokun and Zhao, Tianchen and Wu, Yichen and Zhu, Hongyu and Xie, Ruiqi and Sun, Zhenan and Wang, Yu and Gu, Qingyi},
  journal={arXiv preprint arXiv:2508.03485},
  year={2025}
}
```


# Acknowledgments
Our code was developed based on [ViDiT-Q](https://github.com/thu-nics/ViDiT-Q)(Apache License), 

